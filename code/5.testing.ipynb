{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from models.modeling import VisionTransformer, CONFIGS\n",
    "import copy\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matrics import label_performance, overall_performance, label_performance_old, overall_performance_old, alterationsModel, modelChoices, meanAlterationsAnnotators\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from joblib import dump, load\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some of helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_onboarding = np.load(\"validation_c10n.npy\") # validation images for onboarding. This one has image indexes\n",
    "torch.random.manual_seed(0)\n",
    "transform_test = transforms.Compose([\n",
    "transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=\"./data/\", train=False, download=False, transform=transform_test)\n",
    "clean_testset = torch.tensor(np.array(testset.targets).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def evaluateUserBefore(testloader):\n",
    "    # this method gets the user accuracy before alterations\n",
    "    predictions = torch.tensor([]).to(device)\n",
    "    groundTruth = torch.tensor([]).to(device)\n",
    "\n",
    "    for i, (imgs, labels, n_labels) in enumerate(testloader):\n",
    "        # imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        n_labels = n_labels.to(device)\n",
    "\n",
    "        predictions = torch.cat((predictions, n_labels))\n",
    "        groundTruth = torch.cat((groundTruth, labels))\n",
    "\n",
    "    predictions = predictions.detach().cpu().numpy()\n",
    "    groundTruth = groundTruth.detach().cpu().numpy()\n",
    "\n",
    "    return classification_report(y_true=groundTruth, y_pred=predictions, output_dict=True)\n",
    "\n",
    "def getOnboaringAndTestsets(testset, noiseInjectedTest, validation_onboarding, onboarding_len): \n",
    "    # this method gets the user's test and validation labels and validation set accuracy\n",
    "    validationSet = np.array([random.sample(validation_onboarding[i].tolist(), onboarding_len) for i in range(validation_onboarding.shape[0])]).flatten()\n",
    "    validationSetToRemove = validation_onboarding.flatten()\n",
    "    \n",
    "    noiseInjectedTest = noiseInjectedTest.astype(np.int64)\n",
    "    userNoise = noiseInjectedTest[validationSet] # getting user labels for chosen validation set\n",
    "    val_acc = classification_report(y_true=np.array(testset.targets)[validationSet], y_pred=userNoise, output_dict=True)['accuracy']\n",
    "\n",
    "    testset.data = np.delete(testset.data, validationSetToRemove, axis=0)\n",
    "    testset.targets = np.delete(testset.targets, validationSetToRemove, axis=0)\n",
    "    noiseInjectedTest = np.delete(noiseInjectedTest, validationSetToRemove, axis=0)\n",
    "\n",
    "    class DataWithNoise(torch.utils.data.Dataset):\n",
    "        def __init__(self, dataset, noise):\n",
    "            self._dataset = dataset\n",
    "            self.noise = noise\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self._dataset)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return (*self._dataset[idx], self.noise[idx])\n",
    "    \n",
    "    data_with_noise = DataWithNoise(testset, noiseInjectedTest)\n",
    "    return userNoise, data_with_noise, val_acc\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    #  to evaluate the model/users.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = torch.tensor([]).to(device)\n",
    "        groundTruth = torch.tensor([]).to(device)\n",
    "        n_user = torch.tensor([]).to(device)\n",
    "        n_base = torch.tensor([]).to(device)\n",
    "\n",
    "        for i, (imgs, labels, n_labels) in enumerate(dataloader):\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            n_labels = n_labels.to(device)\n",
    "            n_labels = F.one_hot(n_labels, num_classes=10).to(device, dtype=torch.float32)\n",
    "            \n",
    "            base, user, outputs = model(imgs, n_labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            predictions = torch.cat((predictions, preds))\n",
    "            groundTruth = torch.cat((groundTruth, labels))\n",
    "\n",
    "            n_user = torch.cat((n_user, torch.argmax(n_labels, 1)))\n",
    "            n_base = torch.cat((n_base, torch.argmax(base, 1)))\n",
    "\n",
    "    predictions = predictions.detach().cpu().numpy()\n",
    "    groundTruth = groundTruth.detach().cpu().numpy()\n",
    "    n_user = n_user.detach().cpu().numpy()\n",
    "    n_base = n_base.detach().cpu().numpy()\n",
    "\n",
    "    report = classification_report(y_true=groundTruth, y_pred=predictions, output_dict=True)\n",
    "    # ConfusionMatrixDisplay(confusion_matrix(groundTruth, predictions)).plot()\n",
    "    return report, groundTruth, predictions, n_base, n_user \n",
    "\n",
    "class AdaptedAI(nn.Module): # same model used in training script\n",
    "    def __init__(self):\n",
    "        super(AdaptedAI, self).__init__()\n",
    "\n",
    "        config = CONFIGS['ViT-B_16']\n",
    "        self.base_model = VisionTransformer(config, 224, zero_head=True, num_classes=10)\n",
    "        self.base_model.to(device)\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # to encode the noisy lable\n",
    "        self.n_l_encoder = nn.Sequential(\n",
    "            nn.Linear(10, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10)\n",
    "        )\n",
    "\n",
    "        # input -> img + noisy lable\n",
    "        self.a_ai = nn.Sequential(\n",
    "            nn.Linear(20, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, imgs, n_l):\n",
    "        img_features = self.base_model(imgs)[0] # [0] is because of how vit is designed\n",
    "        n_l_features = self.n_l_encoder(n_l)\n",
    "\n",
    "        out = torch.cat((img_features, n_l_features), dim=1)\n",
    "        out = self.a_ai(out)\n",
    "        return img_features, n_l, out\n",
    "    \n",
    "def loadModel(name):\n",
    "    adapt_model = AdaptedAI().to(device)\n",
    "    for param in adapt_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    adapt_model.load_state_dict(torch.load(name, map_location=torch.device(device)))\n",
    "    return adapt_model\n",
    "\n",
    "def getOurEval(data_records, gt, pred, base_pred, user_pred):\n",
    "    overall, class_result = overall_performance(pred, user_pred, base_pred, gt)\n",
    "\n",
    "    # handling individual class results\n",
    "    temp_label_result = pd.DataFrame([class_result])\n",
    "    temp_label_result.insert(0, 'annot_id', [data_records['annot_id']])\n",
    "    \n",
    "    global class_results\n",
    "    if class_results is None:\n",
    "        class_results = temp_label_result\n",
    "    else:\n",
    "        class_results = pd.concat([class_results, temp_label_result], ignore_index=True)\n",
    "\n",
    "    # handling overall results\n",
    "    data_records.update(overall)\n",
    "    temp_overall = pd.DataFrame([data_records])\n",
    "    temp_overall.fillna(value=0, inplace=True)\n",
    "\n",
    "    global overall_results\n",
    "    if overall_results is None:\n",
    "        overall_results = temp_overall\n",
    "    else:\n",
    "        overall_results = pd.concat([overall_results, temp_overall], ignore_index=True)\n",
    "\n",
    "def recordOutputs(gt, pred, base_pred, user_pred):\n",
    "    # handling raw outputs\n",
    "    outs = {\n",
    "        'gt': gt.astype(int).tolist(),\n",
    "        'base': base_pred.astype(int).tolist(),\n",
    "        'user': user_pred.astype(int).tolist(),\n",
    "        'pred': pred.astype(int).tolist()\n",
    "    }\n",
    "    temp_rawOuts = pd.DataFrame([outs])\n",
    "    global raw_outputs\n",
    "    if raw_outputs is None:\n",
    "        raw_outputs = temp_rawOuts\n",
    "    else:\n",
    "        raw_outputs = pd.concat([raw_outputs, temp_rawOuts], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the svm\n",
    "clf = load('path/to/svm.joblib')\n",
    "\n",
    "# loading cluster wise models (m_theta)\n",
    "clus_model = [ \n",
    "    loadModel(\"path/to/K2C1_model\"), loadModel(\"path/to/K2C2_model\")\n",
    "]\n",
    "\n",
    "# loading test sets for users\n",
    "savedTestUsers = np.load('path/to/testsets.npy') # labels for each test user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate with each test user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_results = None\n",
    "overall_results = None\n",
    "raw_outputs = None\n",
    "\n",
    "for i, annotator in enumerate(savedTestUsers): # iterate through each test user\n",
    "    userNoise, testsetForUser, valAcc = getOnboaringAndTestsets(copy.deepcopy(testset), annotator, validation_onboarding, onboarding_len=20)\n",
    "    testloader = torch.utils.data.DataLoader(testsetForUser, batch_size=512, shuffle=False, num_workers=4)\n",
    "    u_cluster = clf.predict_proba(userNoise.reshape((1,-1)))\n",
    "\n",
    "    clus_p = np.amax(u_cluster, axis=1)[0] # prob of assigning to the cluster\n",
    "    u_cluster = np.argmax(u_cluster, axis=1)[0] # assigned cluster\n",
    "    accuracy_before = evaluateUserBefore(testloader)['accuracy']\n",
    "    report, gt, pred, base_pred, user_pred = evaluate(clus_model[u_cluster], testloader, device)\n",
    "    \n",
    "    data_records = {\n",
    "        'annot_id': i,\n",
    "        'before_acc': accuracy_before,\n",
    "        'val_acc': valAcc,\n",
    "        'clus': u_cluster,\n",
    "        'clus_prob': clus_p,\n",
    "        'after_acc': report['accuracy']\n",
    "    }\n",
    "\n",
    "    recordOutputs(gt, pred, base_pred, user_pred)\n",
    "    getOurEval(data_records, gt, pred, base_pred, user_pred)\n",
    "    print(accuracy_before, report['accuracy'], u_cluster, \"yess\" if accuracy_before<=report['accuracy'] else \"noo\", clus_p )\n",
    "\n",
    "    del testloader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving results\n",
    "# overall_results.to_csv('./eval_c10n_k/overall_k1_vitb16.csv', index=False)\n",
    "# class_results.to_csv('./eval_c10n_k/labelres_k1_vitb16.csv', index=False)\n",
    "# raw_outputs.to_csv('./eval_c10n_k/rawout_k1_vitb16.csv', index=False)\n",
    "\n",
    "# loading results\n",
    "overall_results = pd.read_csv(\"./eval_c10n_k/overall_k3_vitb16.csv\")\n",
    "class_results = pd.read_csv(\"./eval_c10n_k/labelres_k3_vitb16.csv\")\n",
    "raw_outputs = pd.read_csv(\"./eval_c10n_k/rawout_k3_vitb16.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not (0, 21)\n",
      "same (0, 21)\n",
      "improved: (3, 21)\n",
      "before: 0.8570068027210884 after: 0.9898299319727891\n"
     ]
    }
   ],
   "source": [
    "conditions = (overall_results.val_acc < 0.98150) # set base model accuracy here\n",
    "\n",
    "over = overall_results.loc[conditions]\n",
    "print(\"not\", over.loc[(over.before_acc > over.after_acc)].shape)\n",
    "print(\"same\", over.loc[(over.before_acc == over.after_acc)].shape)\n",
    "print(\"improved:\", over.loc[(over.before_acc < over.after_acc)].shape)\n",
    "print(\"before:\", over.before_acc.mean(), \"after:\", over.after_acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Annotators\n",
      "(0.9520010613548688, 0.003973561521271203)\n",
      "(0.9520010613548688, 0.003973561521271203)\n"
     ]
    }
   ],
   "source": [
    "condition2 = overall_results.loc[conditions].annot_id\n",
    "\n",
    "print(\"For Annotators\")\n",
    "print(meanAlterationsAnnotators(class_results.iloc[:, 1:]))\n",
    "print(meanAlterationsAnnotators(class_results.iloc[condition2, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b:c, h:nc, t:c': 3977,\n",
       " 'b:nc, h:c, t:c': 599,\n",
       " 'b:c, h:c, t:c': 24497,\n",
       " 'b:nc, h:nc, t:c': 28,\n",
       " 'b:c, h:nc, t:nc': 98,\n",
       " 'b:nc, h:c, t:nc': 100,\n",
       " 'b:c, h:c, t:nc': 0,\n",
       " 'b:nc, h:nc, t:nc': 101}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get model choices\n",
    "\n",
    "# b -> base model\n",
    "# h -> human\n",
    "# t -> team\n",
    "# \n",
    "# c -> correct\n",
    "# nc -> not correct \n",
    "modelChoices(raw_outputs.iloc[condition2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noisy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
