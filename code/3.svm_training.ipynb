{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import cleanlab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import sklearn_extensions.fuzzy_kmeans as Fuzz\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import dump, load\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import CategoricalNB, GaussianNB\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Cnboarding Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading files saved at the end of clus_prep.ipynb\n",
    "with open('cifar10n_train_clustering_c10n.npy', 'rb') as f:\n",
    "    k_user = np.load(f, allow_pickle=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion to create sub arrays (samples) having a given number of elements from a given array.\n",
    "# this is used to randomly pick instances from a single class\n",
    "def createSubArrays(large_array, n_sub, len_sub):\n",
    "    subarrays = []\n",
    "    for i in range(n_sub):\n",
    "        subarray_indices = np.random.choice(len(large_array), size=len_sub, replace=False)\n",
    "        subarray = large_array[subarray_indices]\n",
    "        subarrays.append(subarray)\n",
    "    return np.array(subarrays)\n",
    "\n",
    "def createDataset(n_samples, sample_len, instance_indexes, user_numLabels, n_aug):\n",
    "    # n_samples - n data points per user\n",
    "    # sample_len - no. of samples to take from 1 class [for a row in data frame]\n",
    "\n",
    "    class_samples = []\n",
    "    for i in range(0,10):\n",
    "        class_samples.append(createSubArrays(instance_indexes[i], n_samples, sample_len))\n",
    "\n",
    "    # combined_samples\n",
    "    # combined in the class order\n",
    "    dataset_with_indexes = []\n",
    "    for i in zip(*class_samples):\n",
    "        dataset_with_indexes.append(np.array(i).flatten())\n",
    "\n",
    "    # X\n",
    "    X = []\n",
    "    Y = []\n",
    "    label = 0\n",
    "    for i in range(0, user_numLabels.shape[0]):\n",
    "        X.append(user_numLabels[i][dataset_with_indexes])\n",
    "        label += 1 if i%n_aug==0 and i>0 else 0\n",
    "        Y.append(np.full(n_samples, label))\n",
    "\n",
    "\n",
    "    data = pd.DataFrame(np.vstack([*X]))\n",
    "    data['label'] = np.hstack([*Y])\n",
    "\n",
    "    # shuffling\n",
    "    # data = data.sample(frac=1)\n",
    "    return data\n",
    "\n",
    "# validation set sampler\n",
    "def create_test_set(n_samples, sample_len, k):\n",
    "    testX = []\n",
    "    testY = []\n",
    "    label = 0\n",
    "    for u in range(0, len(user_numLabels), n_aug):\n",
    "        noise_matrix = np.transpose(np.round(confusion_matrix(gt_numLabels, user_numLabels[u], normalize='true'), decimals=2))\n",
    "        totest = cleanlab.benchmarking.noise_generation.generate_noisy_labels(gt_numLabels, noise_matrix)\n",
    "\n",
    "        class_samples = []\n",
    "        for i in range(0,10):\n",
    "            class_samples.append(createSubArrays(instance_indexes[i], n_samples, sample_len))\n",
    "\n",
    "        dataset_with_indexes = []\n",
    "        for j in zip(*class_samples):\n",
    "            dataset_with_indexes.append(np.array(j).flatten())\n",
    "        \n",
    "        testX.append(totest[dataset_with_indexes])\n",
    "        testY.append(np.full(dataset_with_indexes.__len__(), label))\n",
    "        label += 1\n",
    "\n",
    "\n",
    "    return np.array(testX).reshape((-1, sample_len*10)), np.array(testY).reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=2 # set the selected K\n",
    "# sample_len=20 # no. of samples to take from one class\n",
    "# n_samples=5000 # no. data points per augmentation\n",
    "\n",
    "user_numLabels = k_user[k][:-1] # training labels (augmentations)\n",
    "gt_numLabels = k_user[k][-1] # training consensus\n",
    "\n",
    "# taking indexes of instances from each class\n",
    "instance_indexes = []\n",
    "for i in range(0,10):\n",
    "    instance_indexes.append(np.where(gt_numLabels == i)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k=k, s=n_samples, f=sample_len\n",
    "def saveClusterClassifier(clf, k, n_samples, sample_len):\n",
    "    clf_name = \"svm_k{}_s{}_f{}.joblib\".format(k, n_samples, sample_len)\n",
    "    dump(clf, 'onboarding/'+clf_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the number of times augmented to take samples from every augmentation, and to set the cluster label accurately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples_from_aug: 100 n_samples_from_class: 2 train_shape (600, 21)\n",
      "n_samples_from_aug: 100 n_samples_from_class: 5 train_shape (600, 51)\n",
      "n_samples_from_aug: 100 n_samples_from_class: 10 train_shape (600, 101)\n",
      "n_samples_from_aug: 100 n_samples_from_class: 15 train_shape (600, 151)\n",
      "n_samples_from_aug: 100 n_samples_from_class: 20 train_shape (600, 201)\n"
     ]
    }
   ],
   "source": [
    "n_samples_from_user = []\n",
    "n_samples_from_class = []\n",
    "accuracy = []\n",
    "k_ = []\n",
    "n_aug=3 # set the number of times augmented to take samples from every augmentation, and to set the cluster label accurately\n",
    "\n",
    "# n_samples -> no. data points to sample per user\n",
    "# sample_len -> no. of samples to take from 1 class [for a row in data frame]\n",
    "for k in [2]: # set the K\n",
    "    user_numLabels = k_user[k][:-1]\n",
    "    gt_numLabels = k_user[k][-1]\n",
    "    for n_samples in [100]:\n",
    "        for sample_len in [2, 5, 10, 15, 20]:\n",
    "\n",
    "            # taking indexes of instances from each class\n",
    "            instance_indexes = []\n",
    "            for i in range(0,10):\n",
    "                instance_indexes.append(np.where(gt_numLabels == i)[0])\n",
    "                \n",
    "            dataset= createDataset(n_samples=n_samples, sample_len=sample_len, instance_indexes=instance_indexes, user_numLabels=user_numLabels, n_aug=n_aug)\n",
    "            print(\"n_samples_from_aug:\", n_samples, \"n_samples_from_class:\", sample_len, \"train_shape\", dataset.shape)\n",
    "\n",
    "            clf = make_pipeline(StandardScaler(), SVC(probability=False))\n",
    "            clf.fit(X=dataset.drop('label', axis=1).values, y=dataset['label'])\n",
    "\n",
    "            n_samples_from_user.append(n_samples)\n",
    "            n_samples_from_class.append(sample_len)\n",
    "            k_.append(k)\n",
    "\n",
    "            testset = create_test_set(1000, sample_len, n_aug)\n",
    "            accuracy.append(classification_report(testset[1], clf.predict(testset[0]), output_dict=True)['accuracy'])\n",
    "\n",
    "            # saveClusterClassifier(clf, k, n_samples, sample_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>samples from a cluster</th>\n",
       "      <th>samples from a class</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>15</td>\n",
       "      <td>0.6565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>20</td>\n",
       "      <td>0.6655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   k samples from a cluster  samples from a class  accuracy\n",
       "0  2                    300                     2    0.5360\n",
       "1  2                    300                     5    0.5775\n",
       "2  2                    300                    10    0.6350\n",
       "3  2                    300                    15    0.6565\n",
       "4  2                    300                    20    0.6655"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_comp = pd.DataFrame({\n",
    "    \"k\": k_,\n",
    "    \"samples from a cluster\": (np.array(n_samples_from_user) * n_aug).astype(str),\n",
    "    \"samples from a class\": n_samples_from_class,\n",
    "    \"accuracy\": accuracy})\n",
    "perf_comp.head(10)\n",
    "# perf_comp.to_csv('./onboarding/classifier_stats23287.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noisy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
